input {
  jdbc {
    jdbc_connection_string => "jdbc:mysql://${MYSQL_HOST:cloud-mysql}:${MYSQL_PORT:3306}/${MYSQL_DATABASE:product_db}?useUnicode=true&characterEncoding=utf8&serverTimezone=UTC&useSSL=false&allowPublicKeyRetrieval=true"
    jdbc_user => "${MYSQL_USER:root}"
    jdbc_password => "${MYSQL_PASSWORD:root}"
    jdbc_driver_library => "/usr/share/logstash/drivers/mysql-connector-j-9.3.0.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    schedule => "*/10 * * * * *"
    clean_run => false
    record_last_run => true
    use_column_value => true
    tracking_column => "unix_ts"
    tracking_column_type => "numeric"
    last_run_metadata_path => "/usr/share/logstash/data/.jdbc_products_last_run"
    statement => "
      SELECT
        p.id AS productId,
        p.id AS id,
        p.shop_id AS shopId,
        ms.shop_name AS shopName,
        p.product_name AS productName,
        p.product_name AS productNameKeyword,
        p.price AS price,
        p.stock_quantity AS stockQuantity,
        p.category_id AS categoryId,
        c.name AS categoryName,
        c.name AS categoryNameKeyword,
        p.brand_id AS brandId,
        b.brand_name AS brandName,
        b.brand_name AS brandNameKeyword,
        p.status AS status,
        p.created_at AS createdAt,
        p.updated_at AS updatedAt,
        p.updated_at AS updatedAtSort,
        IFNULL(p.deleted, 0) AS deleted,
        UNIX_TIMESTAMP(p.updated_at) AS unix_ts
      FROM products p
      LEFT JOIN category c ON p.category_id = c.id AND c.deleted = 0
      LEFT JOIN brand b ON p.brand_id = b.id AND b.deleted = 0
      LEFT JOIN merchant_shop ms ON p.shop_id = ms.id AND ms.deleted = 0
      WHERE IFNULL(p.deleted, 0) = 0
        AND UNIX_TIMESTAMP(p.updated_at) > :sql_last_value
      ORDER BY p.updated_at ASC
    "
    jdbc_paging_enabled => true
    jdbc_page_size => 1000
    lowercase_column_names => false
    add_field => { "pipelineSource" => "mysql-product-sync" }
  }

  file {
    path => [
      "/workspace/service-logs/*/*.log",
      "/workspace/service-logs/shared/*.log"
    ]
    start_position => "beginning"
    sincedb_path => "/usr/share/logstash/data/.sincedb_service_logs"
    mode => "tail"
    codec => multiline {
      pattern => "^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\.\\d{3}"
      negate => true
      what => "previous"
    }
    add_field => { "pipelineSource" => "service-log-file" }
  }
}

filter {
  if [pipelineSource] == "mysql-product-sync" {
    mutate {
      convert => {
        "productId" => "integer"
        "shopId" => "integer"
        "categoryId" => "integer"
        "brandId" => "integer"
        "status" => "integer"
        "stockQuantity" => "integer"
        "price" => "float"
        "deleted" => "integer"
        "unix_ts" => "integer"
      }
      remove_field => ["@version", "deleted", "unix_ts", "updatedAtSort"]
    }
  }

  if [pipelineSource] == "service-log-file" {
    grok {
      match => {
        "path" => "/workspace/service-logs/%{DATA:serviceName}/%{DATA:logFileName}"
      }
      tag_on_failure => ["_service_path_parse_failed"]
    }

    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:logTimestamp} \[%{DATA:thread}\] %{LOGLEVEL:level} \[%{DATA:traceId}\] %{DATA:logger} - %{GREEDYDATA:logBody}"
      }
      tag_on_failure => ["_service_log_parse_failed"]
    }

    date {
      match => ["logTimestamp", "yyyy-MM-dd HH:mm:ss.SSS", "ISO8601"]
      target => "@timestamp"
      remove_field => ["logTimestamp"]
    }

    if [logBody] {
      mutate {
        replace => { "message" => "%{logBody}" }
      }
    }

    if ![serviceName] {
      mutate {
        add_field => { "serviceName" => "unknown-service" }
      }
    }

    mutate {
      lowercase => ["serviceName", "level"]
      remove_field => ["host", "@version", "event", "logBody"]
    }
  }
}

output {
  if [pipelineSource] == "mysql-product-sync" {
    elasticsearch {
      hosts => ["http://${ES_HOST:elasticsearch}:${ES_PORT:9200}"]
      index => "product_index"
      document_id => "%{productId}"
      action => "index"
      doc_as_upsert => true
    }
  }

  if [pipelineSource] == "service-log-file" {
    elasticsearch {
      hosts => ["http://${ES_HOST:elasticsearch}:${ES_PORT:9200}"]
      index => "cloud-service-logs-%{+YYYY.MM.dd}"
    }
  }

  stdout {
    codec => rubydebug
  }
}
